{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 快速入门\n",
    "本部分介绍了机器学习中常见任务的API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据\n",
    "PyTorch处理数据有两个基本组件\n",
    "* `torch.utils.data.DataLoader`\n",
    "* `torch.utils.data.Dataset`\n",
    "\n",
    "`Dataset`存储样本及其对应标签，而`DataLoader`将可迭代对象包装在`Dataset`周围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 提供了特定领域的库，如 `TorchText`、`TorchVision` 和 `TorchAudio`，所有这些库都包含数据集。在本教程中，使用一个 `TorchVision` 数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torchvision.datasets`模块包含许多现实世界视觉数据的`Dataset`对象，如 CIFAR、COCO([完整列表在这里](https://pytorch.org/vision/stable/datasets.html))。在这里使用FashionMNIST数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个TorchVision`Dataset`包含两个参数：`transform`和`target_transform`，分别用于修改样本和标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [02:15<00:00, 195118.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 115187.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:02<00:00, 1909830.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 34273455.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 从开放的数据集中下载训练数据集\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor() # 将图像转换成张量的形式\n",
    ")\n",
    "\n",
    "# 从开放的数据集下载测试数据集\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面将`Dataset`作为参数传递给`DataLoader`。这会对我们的数据集进行可迭代包装，并支持自动批处理、采样、洗牌和多进程数据加载。在这里，定义了一个批处理大小为64，级数据加载器可迭代中的每个元素将返回64个特征和标签的批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本X的形状[N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "标签y的形状和数据类型：torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# 创建数据加载器(data loaders)\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# 看一下test_dataloader中的样本和标签的形状\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"样本X的形状[N, C, H, W]: {X.shape}\")\n",
    "    print(f\"标签y的形状和数据类型：{y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型\n",
    "要在PyTorch中定义神经网络，需要创建一个继承`nn.Module`的类。在`__init__`函数中定义网络的层，并在`forward`函数中指定数据如何通过网络传递。为了加速神经网络中的操作，将网络移动到GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可用的设备：cuda\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 为训练分配GPU\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"可用的设备：{device}\")\n",
    "\n",
    "# 定义模型\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # 调用父类的初始化方法\n",
    "        self.flatten = nn.Flatten() # 定义一个Flatten层，用于将输入展平\n",
    "        self.linear_relu_stack = nn.Sequential( # 定义一个顺序容器。包含线性层和ReLU激活函数\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) # 将x进行展平\n",
    "        logits = self.linear_relu_stack(x) # 将展平后的输入通过顺序容器处理\n",
    "        return logits # 返回最终的处理结果logits\n",
    "    \n",
    "\n",
    "model = NeuralNetwork().to(device) # 创建NeuralNetwork的实例，并将其移动到指定的GPU\n",
    "print(model) # 这个是打印模型的结构\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化模型参数\n",
    "要训练一个模型，需要一个损失函数和一个优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # 损失函数使用交叉熵损失函数，也要搞清楚内部的变量\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # 优化函数使用Adam函数，请了解Adam函数内部的变量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在单个的训练循环中，模型对训练数据集（以批次形式输入）进行预测，并反向传播预测误差以调整模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    # 定义一个名为 train 的函数，该函数接受四个参数：\n",
    "    # dataloader: 包含训练数据的加载器，每次迭代提供一个小批量的数据。\n",
    "    # model: 需要训练的神经网络模型。\n",
    "    # loss_fn: 计算模型预测值与真实标签之间误差的损失函数。\n",
    "    # optimizer: 用于更新模型参数的优化器（例如SGD或Adam）。\n",
    "    size = len(dataloader.dataset) # 获取整个训练数据集的大小，存储在变量 size 中。\n",
    "    model.train() # 将模型设置为训练模式。对于某些层（如Dropout和BatchNorm），训练模式和评估模式会有不同的行为。\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 使用 for 循环遍历 dataloader，其中 batch 是当前迭代的索引，(X, y) 是当前小批量的输入数据和对应的标签。\n",
    "        # X: 当前小批量的输入数据。\n",
    "        # y: 当前小批量的真实标签。\n",
    "        X, y = X.to(device), y.to(device) # 将输入数据 X 和标签 y 迁移到指定的计算设备（如GPU或CPU），以加速训练过程。\n",
    "\n",
    "        # 计算预测误差\n",
    "        pred = model(X) # 通过模型进行前向传播计算，得到预测结果 pred。\n",
    "        loss = loss_fn(pred, y) # 使用损失函数计算预测结果 pred 与真实标签 y 之间的误差，并将误差值存储在 loss 中。\n",
    "\n",
    "        # 反向传播更新参数\n",
    "        loss.backward() # 计算损失相对于模型参数的梯度，为反向传播。\n",
    "        optimizer.step() # 使用优化器更新模型参数。\n",
    "        optimizer.zero_grad() # 清空优化器中存储的梯度，防止梯度累积，为下一次迭代做准备。\n",
    "\n",
    "        if batch % 100 == 0: \n",
    "            # 每隔 100 个小批量时，执行打印操作。\n",
    "            # 这里使用取模运算符 % 来判断当前 batch 是否是 100 的倍数。\n",
    "            loss, current = loss.item(), (batch + 1) * len(X) \n",
    "            # 将 loss 转换为 Python 浮点数，并存储在 loss 中。\n",
    "            # 计算当前已经处理的数据量，存储在 current 中。\n",
    "            # (batch + 1) * len(X) 表示已经处理了 (batch + 1) 个小批量，\n",
    "            # 每个小批量包含 len(X) 条数据。\n",
    "            print(f\"损失：{loss:>7f} [{current:>5d} / {size:>5d}]\")\n",
    "            # 打印当前损失值以及训练进度。\n",
    "            # :>7f 表示浮点数右对齐且保留七位小数。\n",
    "            # :>5d 表示整数右对齐，最小宽度为五位。\n",
    "            # 这行代码的输出格式为：损失：XXXX.XXXXXX [YYYYY / ZZZZZ]\n",
    "            # 其中 XXXX.XXXXXX 是当前损失值，YYYYY 是当前已经处理的数据量，\n",
    "            # ZZZZZ 是整个训练数据集的大小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面检查模型在测试数据集上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    # 计算数据集的大小\n",
    "    size = len(dataloader.dataset)\n",
    "    # 计算数据加载器的批次数量\n",
    "    num_batches = len(dataloader)\n",
    "    # 将模型设置为评估模式\n",
    "    model.eval()\n",
    "    # 初始化测试损失和正确预测的数量\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        # 遍历数据加载器中的每个批次\n",
    "        for X, y in dataloader:\n",
    "            # 将输入数据和标签移动到指定设备\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # 使用模型进行预测\n",
    "            pred = model(X)\n",
    "            # 累加损失值\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # 计算正确预测的数量\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    # 计算平均测试损失\n",
    "    test_loss /= num_batches\n",
    "    # 计算平均准确率\n",
    "    correct /= size\n",
    "    # 打印测试误差、准确率和平均误差\n",
    "    print(f\"测试误差：\\n 准确率： {(100 * correct):>0.1f}%, 平均误差: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练过程分为多个迭代（轮次）。在每个轮次中，模型学习参数以做出更好的预测。我们在每个轮次打印模型的准确率和损失；我们希望看到准确率随着每个轮次的增加而提高，损失随着每个轮次的增加而降低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "轮次 1 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 2 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 3 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 4 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 5 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 6 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 7 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 8 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 9 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 10 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 11 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 12 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 13 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 14 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 15 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 16 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 17 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 18 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 19 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "轮次 20 \n",
      " ----------------------------\n",
      "损失：0.083336 [   64 / 60000]\n",
      "损失：0.194866 [ 6464 / 60000]\n",
      "损失：0.179838 [12864 / 60000]\n",
      "损失：0.339022 [19264 / 60000]\n",
      "损失：0.256714 [25664 / 60000]\n",
      "损失：0.330778 [32064 / 60000]\n",
      "损失：0.138128 [38464 / 60000]\n",
      "损失：0.259751 [44864 / 60000]\n",
      "损失：0.149342 [51264 / 60000]\n",
      "损失：0.119077 [57664 / 60000]\n",
      "测试误差：\n",
      " 准确率： 89.1%, 平均误差: 0.479899 \n",
      "\n",
      "训练完成！\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for t in range(epochs):\n",
    "    print(f\"轮次 {t + 1} \\n ----------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn, )\n",
    "\n",
    "print(\"训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存模型\n",
    "常见方法是序列化内部状态字典（包含模型参数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已将 PyTorch 模型状态保存至 model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"已将 PyTorch 模型状态保存至 model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型\n",
    "加载模型的过程包括重新创建模型结构并将状态字典加载其中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在该模型可以用于进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测: \"Ankle boot\", 真实值: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'预测: \"{predicted}\", 真实值: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里了解更多有关保存和加载模型的信息[Saving&Loading_model](https://pytorch.org/tutorials/beginner/basics/saveloadrun_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightning_liang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
